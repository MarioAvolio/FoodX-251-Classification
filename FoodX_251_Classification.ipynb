{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlp2mmlzRGSzj6tr0fkZh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioAvolio/FoodX-251-Classification/blob/main/FoodX_251_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DigOx7Q_MKq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing data augmentation on a batch of images and the need for collate_fn"
      ],
      "metadata": {
        "id": "91qK9qkk2QPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Dataset class, which takes the input images, their classes, and\n",
        "the augmentation object as initializers:"
      ],
      "metadata": {
        "id": "WO4pscnxR6Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch_snippets\n",
        "from torch_snippets import *\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "!pip install torch_summary\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvLDoWI07UIw",
        "outputId": "76159ac0-28aa-46ea-9b1f-0b6060bf98a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_summary in /usr/local/lib/python3.9/dist-packages (1.4.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "class FoodDataset(Dataset):\n",
        "  def __init__(self, x, y, aug=None):\n",
        "    self.y = y\n",
        "    self.x = x \n",
        "    self.aug = aug\n",
        "    self.img_size=(256, 256)\n",
        "    self.transform = transforms.Compose([transforms.Resize(self.img_size),\n",
        "                                      transforms.CenterCrop(self.img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.5], [0.5]),\n",
        "                                      transforms.Lambda(lambda x: x.to(device))])\n",
        "  \n",
        "  def __getitem__(self, ix):\n",
        "    x, y = self.x[ix], self.y[ix]\n",
        "    return x, y\n",
        "\n",
        "  def __len__(self): return len(self.x)\n",
        "\n",
        "  # In general, we leverage the collate_fn method when we have to\n",
        "  # perform heavy computations. This is because performing such\n",
        "  # computations on a batch of images in one go is faster than doing it\n",
        "  # one image at a time.\n",
        "\n",
        "  # Define collate_fn, which takes the batch of data as input:\n",
        "  def collate_fn(self, batch):\n",
        "    # Separate the batch of images and their classes into two different variables\n",
        "    paths, classes = list(zip(*batch))\n",
        "    \n",
        "    # read images\n",
        "    ims = [Image.open(path) for path in paths]\n",
        "\n",
        "    # Specify that augmentation must be done if the augmentation object is\n",
        "    # provided. This is useful is we need to perform augmentation on\n",
        "    # training data but not on validation data\n",
        "    if self.aug: ims=self.aug.augment_images(images=ims)\n",
        "\n",
        "    # Create tensors of images, along with scaling data, by dividing the image shape by 255\n",
        "    ims = torch.tensor(ims)[:,None,:,:].to(device)/255.\n",
        "    classes = torch.tensor(classes).to(device)\n",
        "    return ims, classes"
      ],
      "metadata": {
        "id": "igBy1mhZ1_XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_images = TRAIN_PATH_LOCAL + noise_balanced_dataset.NAME.to_numpy()\n",
        "tr_images "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dFFcpwVe82y",
        "outputId": "df48ca0a-05f7-496f-95c0-ca914067595e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/train_set/train_050876.jpg',\n",
              "       '/content/train_set/train_091456.jpg',\n",
              "       '/content/train_set/train_070941.jpg', ...,\n",
              "       '/content/train_set/train_004212.jpg',\n",
              "       '/content/train_set/train_036549.jpg',\n",
              "       '/content/train_set/train_086447.jpg'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_targets = noise_balanced_dataset.TYPE.to_numpy()\n",
        "tr_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvmeBHIQghY-",
        "outputId": "57840471-6ee6-4e5a-f92f-1a3c0dc364d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([149, 206, 209, ..., 100, 127, 154])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the data augmentation pipeline:"
      ],
      "metadata": {
        "id": "xnTYDfSekRHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imgaug import augmenters as iaa\n",
        "import random\n",
        "\n",
        "def get_random_scale():\n",
        "  return random.uniform(0.5, 1.5)\n",
        "\n",
        "def get_random_translation():\n",
        "  return {'x':random.randint(-50,50),'y':random.randint(-50,50)}\n",
        "\n",
        "aug = iaa.Sequential([\n",
        "iaa.Affine(rotate=(0,360), translate_px=get_random_translation(), scale=get_random_scale(), fit_output=True, mode=\"edge\"),\n",
        "iaa.SaltAndPepper(0.2),\n",
        "iaa.GaussianBlur(sigma=1),\n",
        "# iaa.LinearContrast(0.5),\n",
        "# iaa.Multiply(0.5),\n",
        "])"
      ],
      "metadata": {
        "id": "CyW2sYM0gwig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = FoodDataset(tr_images, tr_targets, aug=aug)"
      ],
      "metadata": {
        "id": "RqsACX5EfSvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the DataLoader, along with the object's\n",
        "collate_fn method, as follows:"
      ],
      "metadata": {
        "id": "r2dpfhirgu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trn_dl = DataLoader(train, batch_size=64, collate_fn=train.collate_fn,shuffle=True)"
      ],
      "metadata": {
        "id": "3Kcybdf4gsZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}