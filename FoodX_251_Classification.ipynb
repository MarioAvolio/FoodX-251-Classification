{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioAvolio/FoodX-251-Classification/blob/main/FoodX_251_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing data augmentation on a batch of images and the need for collate_fn"
      ],
      "metadata": {
        "id": "91qK9qkk2QPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Dataset class, which takes the input images, their classes, and\n",
        "the augmentation object as initializers:"
      ],
      "metadata": {
        "id": "WO4pscnxR6Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch_snippets\n",
        "from torch_snippets import *\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "!pip install torch_summary\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvLDoWI07UIw",
        "outputId": "43f7c9ae-f1f2-43c6-afc0-47b736928c1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch_summary\n",
            "Successfully installed torch_summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s6_-CGMh6f7K"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES=251\n",
        "\n",
        "G_DRIVE='/content/gdrive/'\n",
        "PROJ_PATH= G_DRIVE+\"MyDrive/Visual-Proj/\"\n",
        "\n",
        "# ----------------- tar\n",
        "TRAIN_PATH = PROJ_PATH + \"train.tar\"\n",
        "VAL_PATH = PROJ_PATH + \"val.tar\"\n",
        "VAL_DEGRADED_PATH = PROJ_PATH + \"val_degraded.zip\"\n",
        "NOISE_TRAINING_DATA_PATH = PROJ_PATH + \"noise.zip\"\n",
        "ANNOTATION_PATH = PROJ_PATH + \"annot.tar\"\n",
        "\n",
        "\n",
        "# ---------------- local data extracted\n",
        "TRAIN_PATH_LOCAL=\"/content/train_set/\"\n",
        "VAL_DEGRADED_PATH_LOCAL=\"/content/val_set_degraded/\"\n",
        "VAL_PATH_LOCAL=\"/content/val_set/\"\n",
        "NOISED_PATH_LOCAL=\"/content/noise/\"\n",
        "\n",
        "\n",
        "# --------------- csv file\n",
        "ANNOTATION_PATH_CLASS_LOCAL=\"/content/class_list.txt\"\n",
        "ANNOTATION_PATH_VALIDATION_CLEANED=\"/content/val_info.csv\"\n",
        "ANNOTATION_PATH_TRAIN_CLEANED=PROJ_PATH+\"train_info_cleaned.csv\"\n",
        "ANNOTATION_PATH_BALANCED_TRAIN = PROJ_PATH + \"balanced_train_info.csv\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_from_path(path):\n",
        "  return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)  \n"
      ],
      "metadata": {
        "id": "n-4OUXibOSln"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D1PD4YaP5x5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835128f2-0554-49e7-9ad5-141d1342ab34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount(G_DRIVE, force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wa66t2o87Nfe"
      },
      "outputs": [],
      "source": [
        "!tar -xf $TRAIN_PATH --skip-old-files \n",
        "!tar -xf $VAL_PATH --skip-old-files \n",
        "!tar -xf $ANNOTATION_PATH --skip-old-files \n",
        "!unzip -nqq $VAL_DEGRADED_PATH  \n",
        "!unzip -nqq $NOISE_TRAINING_DATA_PATH  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bkeBNgn1awhd"
      },
      "outputs": [],
      "source": [
        "train_info = pd.read_csv(ANNOTATION_PATH_BALANCED_TRAIN).sample(frac=0.1).reset_index(drop=True) # shuffle\n",
        "name_to_type_train=train_info.set_index(\"NAME\").to_dict()[\"TYPE\"]\n",
        "type_to_name_train = {v: k for k, v in name_to_type_train.items()}\n",
        "# type_to_name_train, train_info"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "21m5KfWKGkZp",
        "outputId": "88aada7a-ca95-4375-fdd0-8d69da21b45c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   NAME  TYPE  NOISED\n",
              "0      train_034633.jpg    95       1\n",
              "1      train_091111.jpg   206       0\n",
              "2      train_111232.jpg   231       0\n",
              "3      train_052880.jpg   116       0\n",
              "4      train_094673.jpg    74       0\n",
              "...                 ...   ...     ...\n",
              "12821  train_085695.jpg     8       0\n",
              "12822  train_029597.jpg    80       0\n",
              "12823  train_101034.jpg   174       0\n",
              "12824  train_042681.jpg    45       0\n",
              "12825  train_016378.jpg    36       0\n",
              "\n",
              "[12826 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52c68355-b92e-4197-9827-45f7c7de75a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>NOISED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_034633.jpg</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_091111.jpg</td>\n",
              "      <td>206</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_111232.jpg</td>\n",
              "      <td>231</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_052880.jpg</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_094673.jpg</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12821</th>\n",
              "      <td>train_085695.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12822</th>\n",
              "      <td>train_029597.jpg</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12823</th>\n",
              "      <td>train_101034.jpg</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12824</th>\n",
              "      <td>train_042681.jpg</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12825</th>\n",
              "      <td>train_016378.jpg</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12826 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52c68355-b92e-4197-9827-45f7c7de75a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52c68355-b92e-4197-9827-45f7c7de75a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52c68355-b92e-4197-9827-45f7c7de75a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B75k8vUXbvU4"
      },
      "outputs": [],
      "source": [
        "colnames=['NAME', 'TYPE'] \n",
        "val_info = pd.read_csv(ANNOTATION_PATH_VALIDATION_CLEANED, index_col=False, header=None, names=colnames)\n",
        "name_to_type_val=val_info.set_index(\"NAME\").to_dict()[\"TYPE\"]\n",
        "type_to_name_val = {v: k for k, v in name_to_type_val.items()}\n",
        "# type_to_name_val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_info = val_info.sample(frac=0.1).reset_index(drop=True) # shuffle"
      ],
      "metadata": {
        "id": "B74PPG3_4uDl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "oMsa4fesA0N_",
        "outputId": "0c9d3dd1-7aca-4572-ac71-ea23143a7203"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                NAME  TYPE\n",
              "0     val_001413.jpg    85\n",
              "1     val_004268.jpg   179\n",
              "2     val_007646.jpg    85\n",
              "3     val_005970.jpg    33\n",
              "4     val_010661.jpg   241\n",
              "...              ...   ...\n",
              "1194  val_000552.jpg   178\n",
              "1195  val_005690.jpg    41\n",
              "1196  val_008170.jpg   112\n",
              "1197  val_009807.jpg    11\n",
              "1198  val_006789.jpg   153\n",
              "\n",
              "[1199 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6628ed88-17fe-4520-b0d9-b1ba4780dd9e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME</th>\n",
              "      <th>TYPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>val_001413.jpg</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>val_004268.jpg</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>val_007646.jpg</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>val_005970.jpg</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>val_010661.jpg</td>\n",
              "      <td>241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>val_000552.jpg</td>\n",
              "      <td>178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>val_005690.jpg</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>val_008170.jpg</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>val_009807.jpg</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>val_006789.jpg</td>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1199 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6628ed88-17fe-4520-b0d9-b1ba4780dd9e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6628ed88-17fe-4520-b0d9-b1ba4780dd9e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6628ed88-17fe-4520-b0d9-b1ba4780dd9e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initilize_path_into_val_dataset(row):\n",
        "  return VAL_PATH_LOCAL + row.NAME"
      ],
      "metadata": {
        "id": "oQTg4Qal9DJu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_info[\"PATH\"] = 0\n",
        "val_info[\"PATH\"] = val_info.apply(initilize_path_into_val_dataset, axis=1)"
      ],
      "metadata": {
        "id": "a2N-p-LS40hF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pK7Jd8LH92Oj",
        "outputId": "ef1163d6-8907-4155-cca1-121267549a78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                NAME  TYPE                             PATH\n",
              "0     val_001413.jpg    85  /content/val_set/val_001413.jpg\n",
              "1     val_004268.jpg   179  /content/val_set/val_004268.jpg\n",
              "2     val_007646.jpg    85  /content/val_set/val_007646.jpg\n",
              "3     val_005970.jpg    33  /content/val_set/val_005970.jpg\n",
              "4     val_010661.jpg   241  /content/val_set/val_010661.jpg\n",
              "...              ...   ...                              ...\n",
              "1194  val_000552.jpg   178  /content/val_set/val_000552.jpg\n",
              "1195  val_005690.jpg    41  /content/val_set/val_005690.jpg\n",
              "1196  val_008170.jpg   112  /content/val_set/val_008170.jpg\n",
              "1197  val_009807.jpg    11  /content/val_set/val_009807.jpg\n",
              "1198  val_006789.jpg   153  /content/val_set/val_006789.jpg\n",
              "\n",
              "[1199 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45c55818-649d-4690-92dc-af4a97c5a447\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>PATH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>val_001413.jpg</td>\n",
              "      <td>85</td>\n",
              "      <td>/content/val_set/val_001413.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>val_004268.jpg</td>\n",
              "      <td>179</td>\n",
              "      <td>/content/val_set/val_004268.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>val_007646.jpg</td>\n",
              "      <td>85</td>\n",
              "      <td>/content/val_set/val_007646.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>val_005970.jpg</td>\n",
              "      <td>33</td>\n",
              "      <td>/content/val_set/val_005970.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>val_010661.jpg</td>\n",
              "      <td>241</td>\n",
              "      <td>/content/val_set/val_010661.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>val_000552.jpg</td>\n",
              "      <td>178</td>\n",
              "      <td>/content/val_set/val_000552.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>val_005690.jpg</td>\n",
              "      <td>41</td>\n",
              "      <td>/content/val_set/val_005690.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>val_008170.jpg</td>\n",
              "      <td>112</td>\n",
              "      <td>/content/val_set/val_008170.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>val_009807.jpg</td>\n",
              "      <td>11</td>\n",
              "      <td>/content/val_set/val_009807.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>val_006789.jpg</td>\n",
              "      <td>153</td>\n",
              "      <td>/content/val_set/val_006789.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1199 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45c55818-649d-4690-92dc-af4a97c5a447')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45c55818-649d-4690-92dc-af4a97c5a447 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45c55818-649d-4690-92dc-af4a97c5a447');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initilize_path_into_dataset(row):\n",
        "  if row.NOISED == 0:\n",
        "    row.PATH = TRAIN_PATH_LOCAL + row.NAME\n",
        "  elif row.NOISED == 1:\n",
        "    row.PATH = NOISED_PATH_LOCAL + row.NAME\n",
        "\n",
        "  return row.PATH"
      ],
      "metadata": {
        "id": "GBmGlL40H7_s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_info[\"PATH\"] = 0\n",
        "train_info[\"PATH\"] = train_info.apply(initilize_path_into_dataset, axis=1)"
      ],
      "metadata": {
        "id": "UP-uM1cIHRnZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tW00p_y6IWia",
        "outputId": "c06b66d7-7e6e-47ad-c15e-29be85350dd8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   NAME  TYPE  NOISED                                 PATH\n",
              "0      train_034633.jpg    95       1      /content/noise/train_034633.jpg\n",
              "1      train_091111.jpg   206       0  /content/train_set/train_091111.jpg\n",
              "2      train_111232.jpg   231       0  /content/train_set/train_111232.jpg\n",
              "3      train_052880.jpg   116       0  /content/train_set/train_052880.jpg\n",
              "4      train_094673.jpg    74       0  /content/train_set/train_094673.jpg\n",
              "...                 ...   ...     ...                                  ...\n",
              "12821  train_085695.jpg     8       0  /content/train_set/train_085695.jpg\n",
              "12822  train_029597.jpg    80       0  /content/train_set/train_029597.jpg\n",
              "12823  train_101034.jpg   174       0  /content/train_set/train_101034.jpg\n",
              "12824  train_042681.jpg    45       0  /content/train_set/train_042681.jpg\n",
              "12825  train_016378.jpg    36       0  /content/train_set/train_016378.jpg\n",
              "\n",
              "[12826 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5d9acea-f31c-482c-827c-73169c9df9aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NAME</th>\n",
              "      <th>TYPE</th>\n",
              "      <th>NOISED</th>\n",
              "      <th>PATH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_034633.jpg</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/noise/train_034633.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_091111.jpg</td>\n",
              "      <td>206</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_091111.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_111232.jpg</td>\n",
              "      <td>231</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_111232.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_052880.jpg</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_052880.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_094673.jpg</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_094673.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12821</th>\n",
              "      <td>train_085695.jpg</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_085695.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12822</th>\n",
              "      <td>train_029597.jpg</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_029597.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12823</th>\n",
              "      <td>train_101034.jpg</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_101034.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12824</th>\n",
              "      <td>train_042681.jpg</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_042681.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12825</th>\n",
              "      <td>train_016378.jpg</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>/content/train_set/train_016378.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12826 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5d9acea-f31c-482c-827c-73169c9df9aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5d9acea-f31c-482c-827c-73169c9df9aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5d9acea-f31c-482c-827c-73169c9df9aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import PIL\n",
        "import torchvision.transforms.functional as fn\n",
        "\n",
        "class FoodDataset(Dataset):\n",
        "  def __init__(self, x, y, aug=None):\n",
        "    self.y = y\n",
        "    self.x = x \n",
        "    self.aug = aug\n",
        "    self.img_size=224\n",
        "    # Normalize does the following for each channel:\n",
        "      # image = (image - mean) / std\n",
        "    self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  def __preprocess_image(self, im):\n",
        "\n",
        "    im = cv2.resize(im, (self.img_size, self.img_size))\n",
        "    \n",
        "    # Specify that augmentation must be done if the augmentation object is\n",
        "    # provided. This is useful is we need to perform augmentation on\n",
        "    # training data but not on validation data\n",
        "    if self.aug: im=self.aug.augment_image(im)\n",
        "\n",
        "    #  While leveraging pre-trained models, it is mandatory to resize,\n",
        "    # permute, and then normalize images (as appropriate for that pretrained model), where the images are first scaled to a value between\n",
        "    # 0 and 1 across the 3 channels and then normalized to a mean of\n",
        "    # [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225]\n",
        "    # across the RGB channels.\n",
        "    im = torch.tensor(im).permute(2,0,1)\n",
        "    im = self.normalize(im/255.)\n",
        "    return im[None]\n",
        "    \n",
        "  def __getitem__(self, ix):\n",
        "    f = self.x[ix]\n",
        "    target = self.y[ix]\n",
        "\n",
        "    im = cv2.imread(f) \n",
        "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    #return im.float().to(device), torch.tensor([target]).float().to(device)\n",
        "    return im, target\n",
        "\n",
        "  def __len__(self): return len(self.x)\n",
        "\n",
        "  # In general, we leverage the collate_fn method when we have to\n",
        "  # perform heavy computations. This is because performing such\n",
        "  # computations on a batch of images in one go is faster than doing it\n",
        "  # one image at a time.\n",
        "\n",
        "  # Define collate_fn, which takes the batch of data as input:\n",
        "  def collate_fn(self, batch):\n",
        "    \"\"\"\n",
        "       batch: is a list of tuples with (example, label)\n",
        "             where 'example' is a tensor of arbitrary shape\n",
        "             and label is scalar\n",
        "    \"\"\"\n",
        "\n",
        "    ims, classes = [], []\n",
        "    for im, target in batch:\n",
        "      im = self.__preprocess_image(im)\n",
        "      ims.append(im)\n",
        "      classes.append(float(target))\n",
        "\n",
        "    classes = [torch.tensor(x).to(device).float() for x in classes]\n",
        "    classes = torch.stack(classes).type(torch.LongTensor).to(device)\n",
        "    ims = torch.cat(ims).to(device)\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Separate the batch of images and their classes into two different variables\n",
        "    # ims, classes = list(zip(*batch))\n",
        "    \n",
        "    # Specify that augmentation must be done if the augmentation object is\n",
        "    # provided. This is useful is we need to perform augmentation on\n",
        "    # training data but not on validation data\n",
        "    # if self.aug: ims=self.aug.augment_images(images=[t.cpu().numpy() for t in ims])\n",
        "\n",
        "    # Create tensors of images, along with scaling data, by dividing the image shape by 255    \n",
        "    # ims = np.array([t.cpu().numpy() for t in ims])\n",
        "    # classes = np.array([t.cpu().numpy() for t in classes])\n",
        "    \n",
        "    # ims = torch.tensor(ims)[:,:,:,:].to(device)#/255.\n",
        "    # classes = torch.tensor(classes).type(torch.LongTensor).to(device)\n",
        "    # -------------------------------------------------\n",
        "    \n",
        "    return ims, classes"
      ],
      "metadata": {
        "id": "igBy1mhZ1_XJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_images = train_info.PATH.to_numpy()\n",
        "tr_images "
      ],
      "metadata": {
        "id": "5dFFcpwVe82y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e95f35-c4d0-4e44-84d2-d5dd3287a520"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/noise/train_034633.jpg',\n",
              "       '/content/train_set/train_091111.jpg',\n",
              "       '/content/train_set/train_111232.jpg', ...,\n",
              "       '/content/train_set/train_101034.jpg',\n",
              "       '/content/train_set/train_042681.jpg',\n",
              "       '/content/train_set/train_016378.jpg'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_targets = train_info.TYPE.to_numpy()\n",
        "tr_targets"
      ],
      "metadata": {
        "id": "uvmeBHIQghY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a62840-dcfa-48ad-8320-16b2157c089f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 95, 206, 231, ..., 174,  45,  36])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_images = val_info.PATH.to_numpy()\n",
        "val_images "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8siVBTfP-ZB",
        "outputId": "3a58fb85-1e00-4df2-94e9-64828db1a894"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['/content/val_set/val_001413.jpg',\n",
              "       '/content/val_set/val_004268.jpg',\n",
              "       '/content/val_set/val_007646.jpg', ...,\n",
              "       '/content/val_set/val_008170.jpg',\n",
              "       '/content/val_set/val_009807.jpg',\n",
              "       '/content/val_set/val_006789.jpg'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_targets = val_info.TYPE.to_numpy()\n",
        "val_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRJhIVi2QEGf",
        "outputId": "b9a73b11-d33a-4f4a-f547-b5a3e7cf2850"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 85, 179,  85, ..., 112,  11, 153])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the data augmentation pipeline:"
      ],
      "metadata": {
        "id": "xnTYDfSekRHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imgaug import augmenters as iaa\n",
        "import random\n",
        "\n",
        "def get_random_scale():\n",
        "  return random.uniform(0.5, 1.5)\n",
        "\n",
        "def get_random_translation():\n",
        "  return {'x':random.randint(-50,50),'y':random.randint(-50,50)}\n",
        "\n",
        "aug = iaa.Sequential([\n",
        "iaa.Affine(rotate=(0,360), translate_px=get_random_translation(), scale=get_random_scale(), fit_output=False, mode=\"constant\"),\n",
        "iaa.SaltAndPepper(0.2),\n",
        "iaa.GaussianBlur(sigma=1),\n",
        "# iaa.LinearContrast(0.5),\n",
        "# iaa.Multiply(0.5),\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CyW2sYM0gwig"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = FoodDataset(tr_images, tr_targets, aug=aug)\n",
        "val = FoodDataset(val_images, val_targets)"
      ],
      "metadata": {
        "id": "RqsACX5EfSvD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the DataLoader, along with the object's\n",
        "collate_fn method, as follows:"
      ],
      "metadata": {
        "id": "r2dpfhirgu7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 32\n",
        "trn_dl = DataLoader(train, batch_size=BATCH, collate_fn=train.collate_fn,shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val, batch_size=BATCH, collate_fn=train.collate_fn)"
      ],
      "metadata": {
        "id": "3Kcybdf4gsZf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = next(iter(trn_dl))\n",
        "print(a.shape, b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "R8DfqRx79lZp",
        "outputId": "208bf493-870a-40ad-9c2b-f5a6c250bb45"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m224\u001b[0m, \u001b[1;36m224\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">224</span><span style=\"font-weight: bold\">])</span>\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">])</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16"
      ],
      "metadata": {
        "id": "seKCLynYP3RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms,models,datasets\n",
        "!pip install torch_summary\n",
        "from torchsummary import summary\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms,models,datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import optim\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "import cv2, glob, numpy as np, pandas as pd\n",
        "from glob import glob\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipGMgs0BP6Jc",
        "outputId": "4406156c-97c7-4f75-e5e8-8a4da9640571"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  model = models.vgg16(pretrained=True).to(device)\n",
        "  # Specify that we want to freeze all the parameters in the model\n",
        "  # downloaded previously\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "  \n",
        "  # Replace the avgpool module to return a feature map of size 1 x 1\n",
        "  # instead of 7 x 7, in other words, the output is now going to be\n",
        "  # batch_size x 512 x 1 x 1:\n",
        "\n",
        "  model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "\n",
        "  # The layer above, nn.AdaptiveAvgPool2d, is yet\n",
        "  # another pooling layer with a twist. We specify the output feature\n",
        "  # map size instead. The layer automatically computes the kernel size\n",
        "  # so that the specified feature map size is returned. For example, if the\n",
        "  # input feature map size dimensions were batch_size x 512 x k\n",
        "  # x k, then the pooling kernel size is going to be k x k. The major\n",
        "  # advantage with this layer is that whatever the input size, the output\n",
        "  # from this layer is always fixed and, hence, the neural network can\n",
        "  # accept images of any height and width.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Define the classifier module of the model, where we first flatten\n",
        "  # the output of the avgpool module, connect the 512 units to the 256\n",
        "  # units, and perform an activation prior to connecting to the output\n",
        "  # layer:\n",
        "  model.classifier = nn.Sequential(nn.Flatten(),\n",
        "                                    nn.Linear(512, 256),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2),\n",
        "                                    nn.Linear(256, 251),\n",
        "                                    nn.Softmax())\n",
        "  \n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr= 1e-3)\n",
        "  return model.to(device), loss_fn, optimizer"
      ],
      "metadata": {
        "id": "4MZpQSHzP-6u"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_summary\n",
        "from torchsummary import summary\n",
        "model, criterion, optimizer = get_model()\n",
        "summary(model, torch.zeros(1,3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MykPTlqQF2i",
        "outputId": "3f043f32-581d-4543-bfb4-379c2668a764"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_summary in /usr/local/lib/python3.10/dist-packages (1.4.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 88.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
            "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        (1,792)\n",
            "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
            "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        (36,928)\n",
            "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
            "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
            "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       (73,856)\n",
            "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
            "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       (147,584)\n",
            "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
            "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
            "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         (295,168)\n",
            "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
            "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         (590,080)\n",
            "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
            "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         (590,080)\n",
            "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
            "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
            "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         (1,180,160)\n",
            "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
            "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         (2,359,808)\n",
            "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
            "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         (2,359,808)\n",
            "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
            "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         (2,359,808)\n",
            "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         (2,359,808)\n",
            "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
            "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         (2,359,808)\n",
            "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
            "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
            "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 1, 1]           --\n",
            "├─Sequential: 1-3                        [-1, 251]                 --\n",
            "|    └─Flatten: 2-32                     [-1, 512]                 --\n",
            "|    └─Linear: 2-33                      [-1, 256]                 131,328\n",
            "|    └─ReLU: 2-34                        [-1, 256]                 --\n",
            "|    └─Dropout: 2-35                     [-1, 256]                 --\n",
            "|    └─Linear: 2-36                      [-1, 251]                 64,507\n",
            "|    └─Softmax: 2-37                     [-1, 251]                 --\n",
            "==========================================================================================\n",
            "Total params: 14,910,523\n",
            "Trainable params: 195,835\n",
            "Non-trainable params: 14,714,688\n",
            "Total mult-adds (G): 15.36\n",
            "==========================================================================================\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 103.36\n",
            "Params size (MB): 56.88\n",
            "Estimated Total Size (MB): 160.82\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 512, 7, 7]           --\n",
              "|    └─Conv2d: 2-1                       [-1, 64, 224, 224]        (1,792)\n",
              "|    └─ReLU: 2-2                         [-1, 64, 224, 224]        --\n",
              "|    └─Conv2d: 2-3                       [-1, 64, 224, 224]        (36,928)\n",
              "|    └─ReLU: 2-4                         [-1, 64, 224, 224]        --\n",
              "|    └─MaxPool2d: 2-5                    [-1, 64, 112, 112]        --\n",
              "|    └─Conv2d: 2-6                       [-1, 128, 112, 112]       (73,856)\n",
              "|    └─ReLU: 2-7                         [-1, 128, 112, 112]       --\n",
              "|    └─Conv2d: 2-8                       [-1, 128, 112, 112]       (147,584)\n",
              "|    └─ReLU: 2-9                         [-1, 128, 112, 112]       --\n",
              "|    └─MaxPool2d: 2-10                   [-1, 128, 56, 56]         --\n",
              "|    └─Conv2d: 2-11                      [-1, 256, 56, 56]         (295,168)\n",
              "|    └─ReLU: 2-12                        [-1, 256, 56, 56]         --\n",
              "|    └─Conv2d: 2-13                      [-1, 256, 56, 56]         (590,080)\n",
              "|    └─ReLU: 2-14                        [-1, 256, 56, 56]         --\n",
              "|    └─Conv2d: 2-15                      [-1, 256, 56, 56]         (590,080)\n",
              "|    └─ReLU: 2-16                        [-1, 256, 56, 56]         --\n",
              "|    └─MaxPool2d: 2-17                   [-1, 256, 28, 28]         --\n",
              "|    └─Conv2d: 2-18                      [-1, 512, 28, 28]         (1,180,160)\n",
              "|    └─ReLU: 2-19                        [-1, 512, 28, 28]         --\n",
              "|    └─Conv2d: 2-20                      [-1, 512, 28, 28]         (2,359,808)\n",
              "|    └─ReLU: 2-21                        [-1, 512, 28, 28]         --\n",
              "|    └─Conv2d: 2-22                      [-1, 512, 28, 28]         (2,359,808)\n",
              "|    └─ReLU: 2-23                        [-1, 512, 28, 28]         --\n",
              "|    └─MaxPool2d: 2-24                   [-1, 512, 14, 14]         --\n",
              "|    └─Conv2d: 2-25                      [-1, 512, 14, 14]         (2,359,808)\n",
              "|    └─ReLU: 2-26                        [-1, 512, 14, 14]         --\n",
              "|    └─Conv2d: 2-27                      [-1, 512, 14, 14]         (2,359,808)\n",
              "|    └─ReLU: 2-28                        [-1, 512, 14, 14]         --\n",
              "|    └─Conv2d: 2-29                      [-1, 512, 14, 14]         (2,359,808)\n",
              "|    └─ReLU: 2-30                        [-1, 512, 14, 14]         --\n",
              "|    └─MaxPool2d: 2-31                   [-1, 512, 7, 7]           --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [-1, 512, 1, 1]           --\n",
              "├─Sequential: 1-3                        [-1, 251]                 --\n",
              "|    └─Flatten: 2-32                     [-1, 512]                 --\n",
              "|    └─Linear: 2-33                      [-1, 256]                 131,328\n",
              "|    └─ReLU: 2-34                        [-1, 256]                 --\n",
              "|    └─Dropout: 2-35                     [-1, 256]                 --\n",
              "|    └─Linear: 2-36                      [-1, 251]                 64,507\n",
              "|    └─Softmax: 2-37                     [-1, 251]                 --\n",
              "==========================================================================================\n",
              "Total params: 14,910,523\n",
              "Trainable params: 195,835\n",
              "Non-trainable params: 14,714,688\n",
              "Total mult-adds (G): 15.36\n",
              "==========================================================================================\n",
              "Input size (MB): 0.57\n",
              "Forward/backward pass size (MB): 103.36\n",
              "Params size (MB): 56.88\n",
              "Estimated Total Size (MB): 160.82\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(x, y, model, opt, loss_fn):\n",
        "  '''\n",
        "  This code passes the batch of images through the model in the\n",
        "  forward pass. It also computes the loss on batch and then passes the\n",
        "  weights through backward propagation and updates them. Finally, it\n",
        "  flushes the memory of the gradient so that it doesn't influence how the\n",
        "  gradient is calculated in the next pass.\n",
        "  '''\n",
        "  model.train() # <- let's hold on to this until we reach dropout section\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # call your model like any python function on your batch\n",
        "  # of inputs\n",
        "  prediction = model(x)\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # compute loss\n",
        "  batch_loss = loss_fn(prediction, y)\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # based on the forward pass in `model(x)` compute all the\n",
        "  # gradients of 'model.parameters()'\n",
        "  batch_loss.backward()\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # apply new-weights = f(old-weights, old-weight-gradients)\n",
        "  # where \"f\" is the optimizer\n",
        "  optimizer.step()\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # Flush gradients memory for next batch of calculations\n",
        "  optimizer.zero_grad()\n",
        "  return batch_loss.item() # Now that we've done this, we can extract the loss value as a scalar by fetching batch_loss.item() on top of batch_loss\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "# since there's no need for updating weights,\n",
        "# we might as well not compute the gradients.\n",
        "# Using this '@' decorator on top of functions\n",
        "# will disable gradient computation in the entire function\n",
        "@torch.no_grad()\n",
        "def accuracy(x, y, model):\n",
        "  \n",
        "  model.eval() # <- let's wait till we get to dropout section\n",
        "\n",
        "  # ----------------------------------\n",
        "  # get the prediction matrix for a tensor of `x` images\n",
        "  prediction = model(x)\n",
        "  # In the following lines of code, we are explicitly mentioning that we don't\n",
        "  # need to calculate the gradient by providing @torch.no_grad() and\n",
        "  # calculating the prediction values by feed-forwarding input through the\n",
        "  # model\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # compute if the location of maximum in each row\n",
        "  # coincides with ground truth\n",
        "  # we invoke prediction.max(-1) to identify the argmax index corresponding to each row.\n",
        "  max_values, argmaxes = prediction.max(-1) \n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  # Furthermore, we are comparing our argmaxes with the ground truth\n",
        "  # through argmaxes == y so that we can check whether each row is\n",
        "  # predicted correctly.\n",
        "  is_correct = argmaxes == y\n",
        "  # ----------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # ----------------------------------\n",
        "  return is_correct.cpu().numpy().tolist() # Finally, we are returning the list of is_correct objects after moving it to a CPU and converting it into a numpy array.\n",
        "  # ----------------------------------\n"
      ],
      "metadata": {
        "id": "kWoxtTNIQgn2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, loss_fn, optimizer = get_model()"
      ],
      "metadata": {
        "id": "fAbmj1dpQaMA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the lists that contain the accuracy and loss values at the end of each epoch\n",
        "train_losses, train_accuracies = [], [] \n",
        "val_accuracies = []\n",
        "\n",
        "\n",
        "for epoch in range(5): # Define the number of epochs\n",
        "  print(f\" epoch {epoch + 1}/5\")\n",
        "\n",
        "  # Invoke the lists that will contain the accuracy and loss values\n",
        "  # corresponding to each batch within an epoch\n",
        "  train_epoch_losses, train_epoch_accuracies = [], []\n",
        "  val_epoch_accuracies = []\n",
        "\n",
        "\n",
        "  # for ix, batch in enumerate(iter(trn_dl)):\n",
        "  #   x, y = batch\n",
        "  #   # y = y.squeeze()\n",
        "  #   # print(len(y))\n",
        "  #   batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "  #   train_epoch_losses.append(batch_loss)\n",
        "\n",
        "  # train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "\n",
        "\n",
        "  for ix, batch in enumerate(iter(trn_dl)):\n",
        "    x, y = batch\n",
        "    is_correct = accuracy(x, y, model)\n",
        "    train_epoch_accuracies.extend(is_correct)\n",
        "\n",
        "  train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        "\n",
        "\n",
        "  for ix, batch in enumerate(iter(val_dl)):\n",
        "    x, y = batch\n",
        "    val_is_correct = accuracy(x, y, model)\n",
        "    val_epoch_accuracies.extend(val_is_correct)\n",
        "  \n",
        "  val_epoch_accuracy = np.mean(val_epoch_accuracies)\n",
        "\n",
        "\n",
        "\n",
        "  train_losses.append(train_epoch_loss)\n",
        "  train_accuracies.append(train_epoch_accuracy)\n",
        "  val_accuracies.append(val_epoch_accuracy)"
      ],
      "metadata": {
        "id": "fG3ZjbIzQePW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "dca91757-5cd4-4927-c194-160ade526f2a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " epoch \u001b[1;36m1\u001b[0m/\u001b[1;36m5\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> epoch <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e5b45ef504f6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mis_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_epoch_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_correct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-3cec0258db4e>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(x, y, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m# compute if the location of maximum in each row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# coincides with ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(5)+1\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "%matplotlib inline\n",
        "plt.plot(epochs, train_accuracies, 'bo',\n",
        "label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'r',\n",
        "label='Validation accuracy')\n",
        "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.title('Training and validation accuracy \\\n",
        "with VGG16 \\nand 1K training data points')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.95,1)\n",
        "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) \\\n",
        "for x in plt.gca().get_yticks()])\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvz1Ng-pMQCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "8ZwVOO8dRK2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}